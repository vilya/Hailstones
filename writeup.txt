Key lessons:
------------
* Try to avoid doing duplicate work
  - Use a cache to avoid recomputing partial sequence lengths.
* Sometimes it's quicker to do unnecessary or duplicate work than it is to do
  all the checks to avoid it.
  - It's quicker to fill the cache completely than to only fill the bits of it
    that we need.
* Avoid branching in inner loops where possible.
  - Replace conditionals with logical operations where possible.
* Exploit the structure of the problem to reduce the amount of calculation
  required.


General approach:
-----------------
I tackle the problem in four distinct phases:

1. Populate a lookup table for the number of trailing zero bits in an unsigned
   byte.
2. Calculate and cache the sequence lengths for the first 2^20 starting
   numbers.
3. Find the counts for each distinct sequence length in the input range.
4. Combine the sequence length counts into buckets as specified on the command
   line.

Only phases 2 and 3 are done in parallel. Phases 1 and 4 are quick enough that
it's not really worthwhile to parallelise them.

Phase 1 calculates a lookup table which we use to speed up calculations in
phase 2 and 3. The observation is that dividing by 2 and incrementing the
length N times is equivalent to dividing by 2^N and adding N to the length.
Dividing by 2^N is the same as right-shifting the number by N places and we can
find N by counting the number of trailing zero bits. The lookup table speeds up
this counting process.

In phase 2 we loop over the range 1 to (1 << 20), calculating sequence lengths
and storing them in a global array. The array index corresponds to the starting
number for the sequence and the value is the corresponding sequence length.

As an optimisation for phase 2, we only do the full length calculation for odd
starting numbers. For each odd number k, we also calculate the length of 2*k as
len(k) + 1; 4*k = len(k) + 2; and so on. In general, length 2^n * k = len(k) +
n, where n is any positive integer. In practice we do this in a loop, doubling
the number each time and incrementing the length by one, until we've exceeded
the upper limit of our range.

Another optimisation is that if, during the calculation we reach a length
greater than the maximum given on the command line, we immediately stop and
return whatever length we've reached. It doesn't matter if this is not exactly
equal to maxLength + 1, just that it's greater than maxLength.

In phase 3 we calculate the sequence lengths for every starting value in the
input range. We use the same doubling optimisation as in phase 2. We also use
the cached sequence lengths to short circuit the calculation: as soon as any
sequence reaches a value less than (1 << 20) we just add the cached sequence 
length for that value to the current sequence length and return.

In this phase we record a count of the number of sequences we encounter for
every possible length. We know thanks to Wikipedia that the longest sequence
for starting values less than 10 billion is only 1132 elements long, so the
counts are stored in an array of that many elements, where the index
corresponds to the sequence length. This makes storing each sequence length
direct array access rather than having to calculate a bucket number and check
for overflow.

This phase is split into two parts: for numbers in the range [lower, lower * 2)
we calculate a sequence length for both even and odd numbers, applying the
repeated doubling optimisation from phase 2 to all of them. For numbers greater
in the range [lower * 2, upper) we've already calculated the lengths for the
even numbers so we just iterate over the odd numbers applying the same
calculations.

Note that although phases 2 and 3 involve similar calculations for the sequence
length, we use a different function in each case. This lets us avoid some
conditional checks which would otherwise slow things down.

Finally, in phase 4, we combine the calculated sequence lengths into buckets.
This a simple serial loop. Doing it as a post-processing step is much more
efficient than doing it while calculating lengths.


Smaller optimisations
---------------------
A few other things that were too small to mention above:

- My compiler (g++) wasn't doing all the optimisations I expected. For
  instance, replacing 3 * n with (n << 1) + 1 actually gave me a slight
  speedup.

- I avoid allocating any memory while processing.

- In general it's best to avoid duplicating work, but sometimes the cost of
  avoiding it is greater than the cost of doing it. So I tested to find the
  sweet spot for the size of the sequence length cache, for example.


What went right
---------------
* Starting with the single threaded version allowed me to check that my code
  was correct. It was also easy to experiment with, so I was able to try out
  various algorithmic improvements quickly and reject the ones that didn't
  work out as well.

* TBB was a good choice. I tried out OpenMP as well and liked it, but I found
  TBB to be better documented and more performant. I never seriously
  considered using raw pthreads or boost::thread.

* Keeping a record of my run times across a few different input ranges allowed
  me to easily (and objectively) check whether any change was really an
  improvement.

* I used size_t variables for everything right from the start. This turned out
  to be a good choice: I later saw reports of people who were getting integer
  overflows due to using 32-bit ints for the intermediate sequence values. It
  does mean that my code wouldn't work correctly on a 32-bit machine, but all
  of the testing machines (and all of my development machines) are 64-bit
  anyway.

* I did a bit of research into hailstone numbers (a.k.a. the Collatz
  conjecture) before getting too far into the implementation. This was very
  helpful in a couple of ways:
  - I found out about some bounds on the problem which I was able to exploit
    for extra speed.
  - It gave me ideas for other optimisations. The wikipedia article suggested
    finding the number of trailing zero bits and right-shifting instead of
    doing multiple divides by two, for example.


What went wrong:
----------------
* I wasted a bit of time chasing down errors because I didn't notice that the
  maximum length was inclusive rather than exclusive. Should have read the
  problem statement more closely the first time through. Sigh.

* I got complacent because of the slow times posted in the forum and wasn't
  going to optimise my program further. At this point it was taking about 10
  seconds to calculate the sequence lengths from 4,000,000,000 to
  4,100,000,000 whereas all the other results posted were over a minute for
  the same range.

  Fortunately, after I posted my times I got a wake up call. I realised that
  some of the earlier posters had improved their times dramatically and that
  there were others who were simply keeping quiet about their times. This
  brought me back into optimising and I found a few very nice wasy to speed
  things up as a result.

* I wasn't able to get onto the MTL for testing, so I was effectively
  submitting my entry blind. I left it quite late in the competition before I
  tried to connect to the MTL; if I had've tried sooner I would have had more
  time to resolve the difficulties and may have been able to resolve them. As
  it was, I ran out of time.

* I'm doing a lot of redundant calculation to set up the sequence length
  lookup table, but I wasn't able to come up with a more efficient way of
  populating it. If I was developing this code further, I would look for ways
  of populating it more sparsely and put in checks to decide whether to use it
  or not.

* I didn't test it thoroughly enough and got caught out by the way TBB splits
  threads. I found out only half an hour before the deadline that I was getting
  incorrect results for very small ranges, where the upper limit was < about
  40. The problem went away when I forced it to run on a single thread, but I
  didn't have time to debug properly so I put in a workaround. I might have
  found this sooner if I'd been able to get onto the MTL for testing, too.

