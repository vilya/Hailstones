Efficient parallel calculation of hailstone number sequence lengths


Key lessons:
------------
* Try to avoid doing duplicate work
  - Use a cache to avoid recomputing partial sequence lengths.
* Sometimes it's quicker to do unnecessary or duplicate work than it is to do
  all the checks to avoid it.
  - It's quicker to fill the cache completely than to only fill the bits of it
    that we need.
* Avoid branching in inner loops where possible.
  - Replace conditionals with logical operations where possible.
* Exploit the structure of the problem to reduce the amount of calculation
  required.
* The compiler may not be doing all the optimisations you expect it to.
* Allowing a benign data race is preferable to requiring locking, but it still
  means you're doing duplicate work so it's better to rearrange your code to
  avoid the race if possible (so long as it doesn't slow things down).


General approach:
-----------------
* Build up a table of trailing zero-bit counts for the numbers 0-255. We use
  this later to perform several steps of the length calculation in one go.
  This is done serially, because it takes almost no time.

* Fill a lookup table of the first 2^20 sequence lengths using the uncached
  sequence length calculation. This is done in parallel using TBB's
  parallel_for construct, as follows:
    For each odd number N from 1 to 2^20
      len <- length of the hailstone sequence starting at N (uncached version)
      tmpN <- N
      while tmpN < 2^20
        cache[tmpN] <- len
        tmpN <- tmpN * 2

* Calculate the sequence lengths for the input range using the cached sequence
  length calculation. This is done in parallel using TBB's parallel_reduce
  construct because we want to combine the sequence lengths counted by each
  thread when they complete.
    For each number N from lower to lower * 2
      len <- length of the hailstone sequence starting at N (cached version)
      tmpN <- N
      while tmpN <= upper
        lengths[tmpN] <- len
        tmpN <- tmpN * 2
    For each odd number N from lower * 2 to upper
      len <- length of the hailstone sequence starting at N (cached version)
      tmpN <- N
      while tmpN <= upper
        lengths[tmpN] <- len
        tmpN <- tmpN * 2

* Combine the sequence lengths into buckets. This is done serially, because
  it's a simple calculation over a small number of values.


Optimisations:
--------------
* While calculating a sequence length, if we pass the maximum length we can
  stop and use whatever length we're currently on. It doesn't have to be
  maxLength + 1 exactly.
  
* Perform multiple divide-by-two steps in one go by finding the number of
  trailing zero bits, adding it to the length and right-shifting by the same
  amount.
  
* Speed up finding the number of trailing zero bits by using a precomputed
  table giving the numbers for 0-255. We mask out the bottom 8 bits of the
  number and use them as an index into the table. Can be repeated if there are
  more than 8 trailing zero bits, but it's quicker to only do it once and
  avoid the conditional.

* Precompute and cache the first 2^20 sequence lengths. When finding the
  lengths for our input range, we check at each step whether the value is less
  than 2^20; if so, we look up the remaining length in the table and return.
  This is based on:
  - Proofs cited on MathWorld and Wikipedia that there are no cycles in the
    sequences for the range we care about, so if we reach a value in our cache
    that's guaranteed to be the correct remaining length.

* Store counts for each individual sequence length while finding all sequence
  lengths for our input range. This means we can defer the bucket and overflow
  calculations until the end, which saves quite a lot of work. We know this is
  safe to do because the longest sequence for values less than 10 billion is
  1132 items (cf. Wikipedia). Our input ranges only go up to 4.2 billion
  (2^32-1 to be precise) so the longest sequence is less than 1132 elements.
  This is confirmed by running the program itself.

* Don't do the full length calculation for even numbers. Instead, iterate over
  all odd numbers and find their length. Then find the lengths of all even
  multiples of each by repeatedly doubling the number and incrementing the
  length.
  - This only works for numbers greater than 2 * the lower bound.
  - You have to calculate sequence lengths for even numbers directly if
    they're less than 2 * the lower bound.

* Separate length-finding functions for use when we're precomputing the lookup
  table vs. when we're finding the lengths to report. This allows us to avoid
  conditional checks in each, making them run faster.

* Multiplying by 3 can be rewritten as a right-shift and an addition. This
  turns out to be faster. I would have expected the compiler to be making
  optimisations like this for me, but it turns out not to (with g++ that is;
  I'd expect the Intel compiler to do better).


What went right
---------------
* Starting with the single threaded version allowed me to check that my code
  was correct. It was also easy to experiment with, so I was able to try out
  various algorithmic improvements quickly and reject the ones that didn't
  work out as well.

* TBB was a good choice. I tried out OpenMP as well and liked it, but I found
  TBB to be better documented and more performant. I never seriously
  considered using raw pthreads or boost::thread.

* Keeping a record of my run times across a few different input ranges allowed
  me to easily (and objectively) check whether any change was really an
  improvement.

* I used size_t variables for everything right from the start. This turned out
  to be a good choice: I later saw reports of people who were getting integer
  overflows due to using 32-bit ints for the intermediate sequence values. It
  does mean that my code wouldn't work correctly on a 32-bit machine, but all
  of the testing machines (and all of my development machines) are 64-bit
  anyway.

* I did a bit of research into hailstone numbers (a.k.a. the Collatz
  conjecture) before getting too far into the implementation. This was very
  helpful in a couple of ways:
  - I found out about some bounds on the problem which I was able to exploit
    for extra speed.
  - It gave me ideas for other optimisations. The wikipedia article suggested
    finding the number of trailing zero bits and right-shifting instead of
    doing multiple divides by two, for example.


What went wrong:
----------------
* I wasted a bit of time chasing down errors because I didn't notice that the
  maximum length was inclusive rather than exclusive. Should have read the
  problem statement more closely the first time through. Sigh.

* I got complacent because of the slow times posted in the forum and wasn't
  going to optimise my program further. At this point it was taking about 10
  seconds to calculate the sequence lengths from 4,000,000,000 to
  4,100,000,000 whereas all the other results posted were over a minute for
  the same range.

  Fortunately, after I posted my times I got a wake up call. I realised that
  some of the earlier posters had improved their times dramatically and that
  there were others who were simply keeping quiet about their times. This
  brought me back into optimising and I found a few very nice wasy to speed
  things up as a result.

* I wasn't able to get onto the MTL for testing, so I was effectively
  submitting my entry blind. I left it quite late in the competition before I
  tried to connect to the MTL; if I had've tried sooner I would have had more
  time to resolve the difficulties and may have been able to resolve them. As
  it was, I ran out of time.

* I'm doing a lot of redundant calculation to set up the sequence length
  lookup table, but I wasn't able to come up with a more efficient way of
  populating it. If I was developing this code further, I would look for ways
  of populating it more sparsely and put in checks to decide whether to use it
  or not.

